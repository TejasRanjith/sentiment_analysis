{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57eecbc",
   "metadata": {},
   "source": [
    "### üìù **Notebook: Toxic Tweet Classification Using TF-IDF and LinearSVC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeea0d1",
   "metadata": {},
   "source": [
    "#### üì¶ **1. Import Required Libraries**\n",
    "- Import all necessary libraries for data handling, NLP, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "540f75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e45fcb",
   "metadata": {},
   "source": [
    "#### üîÉ **2. Download NLTK Resources**\n",
    "- Download resources for tokenization, lemmatization, and stopword removal.\n",
    "- Initialize lemmatizer and stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41ed4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading NLTK resources...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1343d9",
   "metadata": {},
   "source": [
    "#### üìÇ **3. Load Dataset Files**\n",
    "- Load training, test, and sample submission files.\n",
    "- Exit if any file is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c44b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = pd.read_csv('train_2kmZucJ.csv')\n",
    "    test_df = pd.read_csv('test_oJQbWVk.csv')\n",
    "    sample_submission_df = pd.read_csv('sample_submission_LnhVWA4.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Missing file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65e992",
   "metadata": {},
   "source": [
    "#### üßπ **4. Define Preprocessing Functions**\n",
    "- Convert text to lowercase.\n",
    "- Remove URLs, mentions, hashtags, special characters.\n",
    "- Replace slurs.\n",
    "- Tokenize, remove stopwords, lemmatize tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8366a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }.get(tag, wordnet.NOUN)\n",
    "\n",
    "def enhanced_preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r\"@\\w+|#\", '', text)\n",
    "    text = text.replace(\"$&@*#\", \"profane\")\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    tokens = [w for w in word_tokenize(text) if w not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
    "    return \" \".join(lemmatized).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d68b0",
   "metadata": {},
   "source": [
    "#### üîÑ **5. Apply Preprocessing to Dataset**\n",
    "- Clean tweets using the function defined above.\n",
    "- Apply it to both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7527b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing test data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing training data...\")\n",
    "train_df['cleaned_tweet'] = train_df['tweet'].apply(enhanced_preprocess)\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_df['cleaned_tweet'] = test_df['tweet'].apply(enhanced_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbd3a8",
   "metadata": {},
   "source": [
    "#### ‚ú® **6. Vectorize Text using TF-IDF**\n",
    "- Create two TF-IDF vectorizers: one for words and one for character n-grams.\n",
    "- Combine them using `FeatureUnion`.\n",
    "- Fit and transform the training data, transform the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f71100df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text...\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing text...\")\n",
    "word_vect = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_features=15000, sublinear_tf=True, strip_accents='unicode')\n",
    "char_vect = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), max_features=5000, sublinear_tf=True)\n",
    "\n",
    "vectorizer = FeatureUnion([(\"word\", word_vect), (\"char\", char_vect)])\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['cleaned_tweet'])\n",
    "X_test = vectorizer.transform(test_df['cleaned_tweet'])\n",
    "y_train = train_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f561dd",
   "metadata": {},
   "source": [
    "#### ü§ñ **7. Train SVM Model with Cross-Validation**\n",
    "- Use Linear Support Vector Classifier with balanced class weights.\n",
    "- Evaluate using 5-fold cross-validation on F1 (weighted) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b876b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-validation...\n",
      "Weighted F1-scores (CV): [0.88616924 0.89669183 0.89535764 0.88366326 0.88435273]\n",
      "Mean Weighted F1-score: 0.8892\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C=0.5, class_weight='balanced', random_state=42, max_iter=5000)\n",
    "\n",
    "print(\"Performing cross-validation...\")\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "print(f\"Weighted F1-scores (CV): {cv_scores}\")\n",
    "print(f\"Mean Weighted F1-score: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14705d17",
   "metadata": {},
   "source": [
    "#### ‚úÖ **8. Train Final Model and Predict**\n",
    "- Train final model on the full training set.\n",
    "- Make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f72b8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model...\n",
      "Predicting on test data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training final model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Predicting on test data...\")\n",
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a1173",
   "metadata": {},
   "source": [
    "#### üìÑ **9. Save Predictions to Submission File**\n",
    "- Create submission file in required format and save it as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "032ea01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_tuned.csv' created.\n",
      "     id  label\n",
      "0  7921      1\n",
      "1  7922      1\n",
      "2  7923      1\n",
      "3  7924      1\n",
      "4  7925      1\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': test_predictions})\n",
    "submission_df.to_csv('submission_tuned.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_tuned.csv' created.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce4ae3",
   "metadata": {},
   "source": [
    "#### üìä **10. Evaluate F1 Score on Training Data**\n",
    "- Use model predictions on training set to calculate F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbc097b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score (Weighted): 0.9694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_preds, average='weighted')\n",
    "print(f\"Training F1 Score (Weighted): {train_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178cd54",
   "metadata": {},
   "source": [
    "#### üìÅ **11. Read and Display Submission CSV**\n",
    "- Read the saved `submission_tuned.csv` to verify contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41c135e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  label\n",
       "0  7921      1\n",
       "1  7922      1\n",
       "2  7923      1\n",
       "3  7924      1\n",
       "4  7925      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_check = pd.read_csv('submission_tuned.csv')\n",
    "submission_check.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
